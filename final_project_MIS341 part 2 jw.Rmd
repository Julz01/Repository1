---
title: "Final Project"
output:
  pdf_document: default
  word_document: default
---


**Julien Ward**:
****:


```{r warning = FALSE, message = FALSE}
# Suppress dplyr summarise grouping warning messages
options(dplyr.summarise.inform = FALSE)

## Add R libraries here
library(tidyverse)
library(tidymodels)
library(vip)
library(parsnip)
library(discrim)
library(klaR)
library(MASS)
library(rpart)


# Load data

loans_df <- read_rds("loan_data(1).rds")

```


# Data Analysis [30 Points]

In this section, you must think of at least 5 relevant questions that explore the relationship between `loan_default` and the other variables in the `loan_df` data set. The goal of your analysis should be discovering which variables drive the differences between customers who do and do not default on their loans.

You must answer each question and provide supporting data summaries with either a summary data frame (using `dplyr`/`tidyr`) or a plot (using `ggplot`) or both.

In total, you must have a minimum of 3 plots (created with `ggplot`) and 3 summary data frames (created with `dplyr`) for the exploratory data analysis section. Among the plots you produce, you must have at least 3 different types (ex. box plot, bar chart, histogram, scatter plot, etc...)

See the example question below.


**Note**: To add an R code chunk to any section of your project, you can use the keyboard shortcut `Ctrl` + `Alt` + `i` or the `insert` button at the top of your R project template notebook file.


## Sample Question

**Are there differences in loan default rates by loan purpose?**

**Answer**: Yes, the data indicates that credit card and medical loans have significantly larger default rates than any other type of loan. In fact, both of these loan types have default rates at more than 50%. This is nearly two times the average default rate for all other loan types.


### Summary Table

```{r echo = TRUE, fig.height=5, fig.width=9}
loans_df %>%
  group_by(loan_purpose) %>% 
  summarise(n_customers = n(),
            customers_default = sum(loan_default == 'yes'),
            default_percent = 100 * mean(loan_default == 'yes'))
```


### Data Visulatization

```{r echo = TRUE, fig.height=5, fig.width=9}
default_rates <- loans_df %>%
                 group_by(loan_purpose) %>% 
                 summarise(n_customers = n(),
                 customers_default = sum(loan_default == 'yes'),
                 default_percent = 100 * mean(loan_default == 'yes'))
default_rates


ggplot(data = default_rates, mapping = aes(x = loan_purpose, y = default_percent)) +
    geom_bar(stat = 'identity', fill = '#006EA1', color = 'white') +
    labs(title = 'Loan Default Rate by Purpose of Loan',
         x = 'Loan Purpose',
         y = 'Default Percentage') +
    theme_light()
```




## Question 1


**Question**: Is there a skew in proportion of defaults across annual income? 


**Answer**:
Yes, the histogram shows a right skew for the proportion of defaults by annual income. This shows that there is a correlation between higher incomes and lower default rates. The highest proportion of defaults is noted at approximately $50,000 at a rate of 120 defaults, whereas at an income of $200,000, the default rate is less than 1/8 of the former.

```{r}
default_rates_income <- loans_df %>%
                 group_by(annual_income) %>% 
                 summarise(n_customers = n(),
                 customers_default = sum(loan_default == 'yes'),
                 default_percent = 100 * mean(loan_default == 'yes'))
default_rates_income

ggplot(default_rates_income, aes(x = annual_income)) + 
  geom_histogram(bins = 15) + labs(title = "Density Histogram of default rate by annual income", y = "proportion of defaults")
```




## Question 2


**Question**: Are there differences in default rates by application type and average loam amount?


**Answer**:
The table shows that there is a notable difference in default percentages by application type. joint applications make up only ~15% of the total applications, yet they account for a hhigher percentage of defaulting. Along with this, join applications have a higher mean loan amount in total than individual applications. 



```{r}
default_rates_app_type <- loans_df %>%
                 group_by(application_type) %>% 
                 summarise(n_customers = n(),
                 customers_default = sum(loan_default == 'yes'),
                 default_percent = 100 * mean(loan_default == 'yes'),
                 mean_loan_amount = mean(loan_amount))
default_rates_app_type

616/(616+3494)
```




## Question 3


**Question**: IS there a different skew in interest rates across annual income between loan default statuses?


**Answer**:
No, surprisingly there seems to be a similar skew rate of interest and income between loan default statuses. With this, we can note that having a higher annual income does not seem to impact the overall skew in load defaults in relation to interest rates. It is noteworthy that there are higher "no" loan default statuses with higher incomes however.  

```{r}
ggplot(data = loans_df, mapping = aes(x = annual_income, y = interest_rate)) + 
  geom_col(mapping = aes(color = loan_default)) +
  facet_wrap(~ loan_default, nrow = 2) 
  


```



## Question 4


**Question**: Is there a linear trend across loan amounts by annual income within each homeownership category for loan default statuses?


**Answer**:
No, there is absolutely no trend between annual income and loan amounts that could point to differences between defaulting on loans or not- including across homeownership statuses.

```{r}
loans_df %>%
   mutate(default_percent = 100 * mean(loan_default == 'yes'))
       
ggplot(loans_df, aes(x = loan_amount, y = annual_income)) + geom_point(alpha = 0.25, color = "#028000") + facet_grid(homeownership ~ loan_default) + geom_smooth(mapping = aes(x = loan_amount, y = annual_income))


```




## Question 5


**Question**: Is there a skew in proportion of default rates by debt to income?


**Answer**:
Yes, there is a significant right skew for default rates by debt to income in the following histogram.

```{r}
default_rates_debt <- loans_df %>%
                 group_by(debt_to_income) %>% 
                 summarise(n_customers = n(),
                 customers_default = sum(loan_default == 'yes'),
                 default_percent = 100 * mean(loan_default == 'yes'))
default_rates_debt

ggplot(default_rates_debt, aes(x = debt_to_income)) + 
  geom_histogram(bins = 15) + labs(title = "Density Histogram of default rate by debt to income
", y = "proportion of defaults")
```



# Predictive Modeling [70 Points]


In this section of the project, you will fit **two classification algorithms** to predict the response variable,`loan_default`. You should use all of the other variables in the `loans_df` data as predictor variables for each model.

You must follow the machine learning steps below. 

The data splitting and feature engineering steps should only be done once so that your models are using the same data and feature engineering steps for training.

- Split the `loans_df` data into a training and test set (remember to set your seed)
- Specify a feature engineering pipeline with the `recipes` package
    - You can include steps such as skewness transformation, dummy variable encoding or any other steps you find appropriate
- Specify a `parsnip` model object
    - You may choose from the following classification algorithms:
      - Logistic Regression
      - LDA
      - QDA
      - KNN
      - Decision Tree
      - Random Forest
- Package your recipe and model into a workflow
- Fit your workflow to the training data
    - If your model has hyperparameters:
      - Split the training data into 5 folds for 5-fold cross validation using `vfold_cv` (remember to set your seed)
      - Perform hyperparamter tuning with a random grid search using the `grid_random()` function 
      - Hyperparameter tuning can take a significant amount of computing time. Be careful not to set the `size` argument of `grid_random()` too large. I recommend `size` = 10 or smaller.
      - Select the best model with `select_best()` and finalize your workflow
- Evaluate model performance on the test set by plotting an ROC curve using `autoplot()` and calculating the area under the ROC curve on your test data

```{r}
# setting seed and splitting data
set.seed(123)
loan_split <- initial_split(loans_df, prop = 0.75,
strata = loan_default)

loan_training <- loan_split %>%
  training()

loan_test <- loan_split %>% 
  testing()

# Create folds for cross validation on the training data set
## These will be used to tune model hyperparameters for Model 3
set.seed(123)
loan_folds <- vfold_cv(loan_training, v = 5)

```
```{r}
# creating the recipe for the loan data
loan_recipe <- recipe(loan_default ~ ., data = loan_training) %>%
step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
step_normalize(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes())

summary(loan_recipe)

loan_recipe

loan_recipe %>%
prep() %>%
bake(new_data = loan_training)
```



## Model 1
```{r}
# specification for the logistical regression
logistic_model <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
logistic_model
```
```{r}
# create the workflow for logistic model
loan_wf <- workflow() %>%
  add_model(logistic_model) %>%
  add_recipe(loan_recipe)

loan_wf
```

```{r}
# Fit the model to Training Data
loan_logistic_fit <- loan_wf %>%
fit(data = loan_training)
# View heart_logistic_fit properties
loan_logistic_fit
```

```{r}
# predicting categories then probabilities then combining
predictions_categories <- predict(loan_logistic_fit, new_data = loan_test)

# predicting the probabilities
predictions_probabilities <- predict(loan_logistic_fit, new_data = loan_test, type = 'prob')
 
# Combine
test_results <- loan_test %>% 
 # select(loan_default) %>%
  bind_cols(predictions_categories) %>%
  bind_cols(predictions_probabilities)

test_results
```
```{r}
# making a confusion matrix to evaluate the model's true and false positives and negatives
conf_mat(test_results, truth = loan_default, estimate = .pred_class)

```
We have 976 correct predictions, 21 false positives, and 31 false negatives with this model. 
```{r}
# F1 score
f_meas(test_results, truth = loan_default, estimate = .pred_class)
```

```{r}
# ROC curve
roc_curve(test_results, truth =
loan_default, estimate = .pred_yes)
# Plotting the ROC curve
roc_curve(test_results, truth = loan_default, estimate = .pred_yes) %>% 
  autoplot()

```

```{r}
# Area under the ROC curve
roc_auc(test_results, truth = loan_default, .pred_yes)


```




## Model 2
## CODE FOR MODEL 2 
```{r}
# Model Specification
lda_model <- discrim_regularized(frac_common_cov = 1) %>%
set_engine('klaR') %>%
set_mode('classification')

lda_model
```

```{r}
#Create Workflow 
lda_wf <- workflow() %>%
add_model(lda_model) %>%
add_recipe(loan_recipe)
lda_wf
```

```{r}
#Train and evaluate with last_fit()
last_fit_lda <- lda_wf %>%
last_fit(split = loan_split)
```

```{r}
# Accuracy and Area under the ROC curve
last_fit_lda %>%
collect_metrics()

```

```{r}
# ROC Curve
lda_predictions <- last_fit_lda %>%
collect_predictions()
lda_predictions
```

```{r}
lda_predictions %>% roc_curve(truth =
loan_default, estimate = .pred_yes) %>%
autoplot()


```
```{r}
# Confusion Matrix
conf_mat(lda_predictions, truth = loan_default,
estimate = .pred_class)

```
```{r}
#F1 Score
f_meas(lda_predictions, truth = loan_default,
estimate = .pred_class)
```

#CODE FOR MODEL 3
```{r}
# Create folds for cross validation on the training data set
## These will be used to tune model hyperparameters
# set.seed(314)
# churn_folds <- vfold_cv(churn_training, v = 5)
# THE ABOVE CODE WAS ALREADY USED AT THE START OF PART 2, BUT I AM PUTTING IT HERE IN COMMENTS TO NOTE THAT IT REFERS TO MY MODEL 3 DECISION TREE.

#model specification
tree_model <- decision_tree(cost_complexity = tune(),
tree_depth = tune(), min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('classification')
```
```{r}
tree_workflow <- workflow() %>%
add_model(tree_model) %>%
add_recipe(loan_recipe)
```
```{r}
# Create a grid of hyperparameter values to test
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
min_n(),
levels = 2)
# View grid 
tree_grid
```
```{r}
## Tune decision tree workflow
set.seed(123)
tree_tuning <- tree_workflow %>%
tune_grid(resamples = loan_folds, grid = tree_grid)
```
```{r}
## Show the top 5 best models based on roc_auc metric
tree_tuning %>% show_best('roc_auc')
```
```{r}
## Select best model based on roc_auc
best_tree <- tree_tuning %>% 
  select_best(metric = 'roc_auc')

best_tree
```
```{r}
# finalizing workflow with best model
final_tree_workflow <- tree_workflow %>%
finalize_workflow(best_tree)

```
```{r}
# Fit the model
tree_wf_fit <- final_tree_workflow %>%
fit(data = loan_training)

# Explore the trained model
tree_fit <- tree_wf_fit %>%
pull_workflow_fit()
```
```{r}
vip(tree_fit)
```
```{r}
# rpart.plot(tree_fit$fit, roundint = FALSE)
```
```{r}
# Train and evaluate with last_fit()
tree_last_fit <- final_tree_workflow %>%
last_fit(loan_split)
```
```{r}
# Accuracy and Area under the ROC curve
tree_last_fit %>%
collect_metrics()
```
```{r}
# Estimated probabilities
tree_predictions <- tree_last_fit %>%
collect_predictions()
tree_predictions
```

## Model 3

```{r}
#ROC Curve
tree_predictions %>% roc_curve(truth = loan_default, estimate = .pred_yes) %>% 
  autoplot()


```
```{r}
# Confusion Matrix
conf_mat(tree_predictions, truth = loan_default, estimate = .pred_class)
```


# Summary of Results [50 Points]

Write a summary of your overall findings and recommendations to the executives at the bank. Think of this section as your closing remarks of a presentation, where you summarize your key findings, model performance, and make recommendations to improve loan processes at the bank.

Your executive summary must be written in a business tone, with minimal grammatical errors, and should include the following sections:

1. An introduction where you explain the business problem and goals of your data analysis

    - What problem(s) is this company trying to solve? Why are they important to their future success?
  
    - What was the goal of your analysis? What questions were you trying to answer and why do they matter?


2. Highlights and key findings from your Exploratory Data Analysis section 
    - What were the interesting findings from your analysis and **why are they important for the business**?

    - This section is meant to **establish the need for your recommendations** in the following section


3. Your “best” classification model and an analysis of its performance 
    - In this section you should talk about the expected error of your model on future data
      - To estimate future performance, you can use your model performance results on the **test data**
    - You should discuss at least one performance metric, such as an F1 or ROC AUC for your model. However, you must explain the results in an **intuitive, non-technical manner**. Your audience in this case are executives at a bank with limited knowledge of machine learning.


4. Your recommendations to the company on how to reduce loan default rates 
  
    - Each recommendation must be supported by your data analysis results 

    - You must clearly explain why you are making each recommendation and which results from your data analysis support this recommendation

    - You must also describe the potential business impact of your recommendation:
      
      - Why is this a good recommendation? 
      
      - What benefits will the business achieve?

5. Conclusion

Wrap up the report with concluding remarks by summarizing the results and your recommendations in one or two paragraphs.



--- End of the Project ---
